---
layout: post
title: >
    Julia plotter: Baby's third compiler
title-lower: >
    julia plotter: baby's third compiler
title-tiny: julia plotter
blurb: Parsing math into GLSL to render Julia sets.
usemathjax: true
---
{::nomarkdown}

[block]
So, the first very iteration of this website had this "Julia plotter" thingamajig. The code's very old already and pretty darn bad, but if I'm going to write about compilers on this blog, I might as well start with something small and easy. And easy it was -- I had already made two other small compilers already. If my memory's to be trusted, this took an afternoon.

In this post, I'll discuss this plotter, why you shouldn't care, the code behind it, and why you should care. I'll go through everything this compiler needed to work. Now, without further ado, let's first take a look at the plotter:
[/block]

[block]
{% include julia.html %}
[/block]

[block]
In the above visualization, you can write some math[^1], and then the plotter plots its *Julia set*. You can zoom in as you'd expect, and you can focus the image around a different point by clicking there. If your input's doing something weird, you'll even get a very vague error message!

Julia sets
==========
Julia sets are a mathematical curiosity part of chaos theory. Consider the sequence $(z, f(z), f(f(z)), f(f(f(z))), \dots)$, where $f$ is a user-specified function. This plotter then simply counts how long it takes each starting point before you're "far" from the origin, and colours each pixel based on that.

The motivation for this is that you'll see only two behaviours for certain functions: your starting point either blasts away to wherever, or it stabilizes. Then, once you're far enough away from the origin, you're going to go infinitely far out. I'm completely ignoring this "certain" keyword however, and just letting you use whatever function. As a result, this thing is provably reasonably accurate for all functions $f(z) = z^2 + c$, but all bets are off for anything else. Therefore, the actual mathematical value of this plotter is somewhat limited. Just consider it a "funky picture generator".

If you want to learn more about Julia sets, the [Wikipedia](https://en.wikipedia.org/wiki/Julia_set) page is pretty understandable, or you could just grab a good ol' textbook[^2].

Compilers
=========
With that out of the way, time to get to the point of this post: how I made this. The idea is simple: just write some shader that does the iteration, and depending on how many iterations it takes, give it a different colour.

```glsl
// For each pixel...
void main() {
    // Prep work: handle the viewport to find the what number
    // this coordinate represents.
    vec2 z = vPosition.xy * exp(zoom) - offset;
    z.x *= aspectRatio;

    // Actually do the iteration.
    // (Stop after some limit, and consider that ∞.)
    float iters = 0.;
    for (float i = 0.; i < 100.; i++) {
        z = /* f(z) -- put compiled user code here */;
        iters += float(dot(z,z) < 4.);
    }

    // Colour based on the amount of iterations.
    // (This `vSampler` contains a linear Viridis scale.
    //  I liked a logarithmic scale more.)
    float progress = log2(iters + 2.)/log2(102.);
    gl_FragColor = texture2D(vSampler, vec2(progress,0.5));
}
```

What we have to do is not simple, but "simple". We have to take some math that the user wrote, and turn it into valid shader code. This is exactly what a compiler/interpreter is for.

Generally, the work of compilers/interpreters can be roughly summarised in a few steps:
- First, *tokenization*: take the user's text, and turn it into tokens with a *lexer*[^3]. Instead of working character per character, this allows us to work with significant text blocks. For instance a full `function` may be one token, just like a single mathematical operator `-`. This step only exists to make our lives easier.

    This can be done at the same time as the next step if you're adventurous, but I won't recommend that[^4].
- Next, *parsing*: turn this list of tokens into *syntax tree*. A syntax tree is the most natural way to analyse a bunch of code; you can ask operators what their operands are, you can ask function calls what their arguments are, etc.

    IMAGE

    By doing this, you don't have to care about the textual representation of the code, which is a godsent. Imagine having to account for whitespace every time you wanted to loop over a function's arguments!
- Optionally, you can *analyse* en *optimize* the resulting syntax tree. This is usually where the "meat" of a compiler is[^5]. You can do type checking, constant folding, the possibilities are endless.

    For example, while the following two syntax trees mean the same, one is clearly more efficient than the other. Naturally, you'd like to always get the more efficient tree:

    IMAGE

    For this plotter, my optimizations consist of "n/a", while my analysis consists of determining whether we're working with real, or with complex numbers, in every step. For instance, the "sign" function only makes sense on real numbers, so if the user writes down $\text{sgn}(i)$, we should tell them "no can do".
- The next step depends on whether you're a compiler or an interpreter. If you're an interpreter, you walk the syntax tree, executing nodes as you encounter them. If you're a compiler, you take the syntax tree, and turn it back into text, bytcode, or whatever else is appropriate.

    Depending on the complexity of your language(s), this step is either very easy or very hard. In most cases, it's the latter. Luckily, in our case, it's the former.

Let's go through these steps one by one.

Tokenization
============
Before we can turn our text into meaningful chunks, we need to decide what those meaningful chunks even are. Of course, we have our single variable `z`, the complex unit `i`, and numbers `3.14`. We also want to allow arithmetic such as `+` or `-`. Functions such as `sin`, `re` also require us to allow for `()` parentheses and `,` comma's. Whitespace separates tokens. This already gives us our full set of allowed tokens.

Now that we have specified what we allow, there is really nothing interesting going on. Consume characters, one at a time, and determine fairly ad-hoc what the recent batch of characters looks like.

In my case, we first do some clean-up before lexing to allow just a slight bit more. Both `log` and `ln` are a common way to denote the same function, so replace the one with the other. Similarly, `**` and `^` are both common exponentiations, so also unify those. Absolute values `|expression|` can be turned into `abs(expression)` with a simple regex, and we also convert `pi` to its numerical value while we're at it.

Actually consuming the characters is very simple with our lexer. All of `+-*/^(),` are always a single token. The only things remaining are numbers and letter sequences. There's only a few edge cases to consider.
- Mathematicians like not writing the multiplication sign. If we find something like `2i`, we will actually output the tokens `Number(2)`, `Op(*)`, `i`. Parentheses have a similar problem.
- Letter sequences are always function calls, except when they consist only of `i` and `z` characters. These also get the implicit multiplication token.

Taking into account these things is arguably not the lexer's job, but the parser's, as we're inserting characters that didn't exist. However, doing it in the lexing phase is just easier.

In all, in pseudo-pseudocode, tokenization will look something like the following.
```js
let tokens = []
while (peek()) {
    // Whitespace is insignificant (other than separating tokens).
    // Ignore it.
    while (peek() === " ")
        consume();

    let col = get_current_pos();
    let c = consume();
    // First, single-character tokens.
    if (!!ops[c]) { // "ops" contains all op properties by character.
        tokens.push(new Token(col, TOKEN.Op, c));
        continue;
    }
    switch(c) {
        case ",": tokens.Push(new Token(col, TOKEN.Comma)); continue;
        case "(": tokens.Push(new Token(col, TOKEN.Open));  continue;
        case ")":
            tokens.Push(new Token(col, TOKEN.Close));
            // If an implicit multiplication follows, insert it.
            // These are )i, )log, )(.
            if (peek().match(/[a-z]|\(/g))
                tokens.Push(new Token(col, TOKEN.Op, "*"));
            continue;
    }

    // Now, multi-character tokens.
    if (is_number(c)) {
        let start_col = col;
        let num = consume_number();
        tokens.push(new Token(start_col, TOKEN.Num, num));
        // If an implicit multiplication follows, insert it.
        // These are 2i, 2log, 2(
        if (peek().match(/[a-z]|\(/g))
            tokens.push(new Token(col, TOKEN.Op, "*"));
        continue;
    }

    if (is_letter(c)) {
        // You can imagine the rest.
    }
}
```
I've swept quite some functions under the rug, but they're mostly self-explanatory. `peek()` looks at the next character, while `consume()` consumes the next character, advancing us further into the code.

After lexing, we end up with a list of tokens. We now need to use these to actually see what our code is doing.

Parsing
=======
The idea behind parsing is simple. Just like how we peeked and consumed character by character when lexing, we will now consume token by token when parsing. We will just keep calling nested `parse_XXX` functions for whatever is appropriate, letting recursion handle any difficulties in ordering. For instance, a method for parsing a function call might look as follows.

```js
function parse_function() {
    let name = consume().value; // The function's identifier name.
    let sig = functions[name].sig; // Method signature in `functions`.
    let col = get_current_pos();

    consume_expected(TOKEN.Open); // The function's opening (.

    // Eat the arguments one by one.
    let args = [];
    for (let i = 0; i < sig.arg_count; i++) {
        // We don't care about the specifics!
        // Any math is allowed at this point, so just parse that.
        args.push(parse_math());
        if (i < sig.arg_count - 1)
            consume_expected(TOKEN.Comma); // Arg separation.
    }

    // Final touches.
    consume_expected(TOKEN.Close);
    return new ASTFunction(col, name, args);
}

// Consumes the next token, and errors when it's not what we expect.
// Allows both a single item, or a list. When it's a list, we need to
// match any in the list.
function consume_expected(tokentypes) {
    let token = consume();
    if (token === tokentypes || tokentypes.indexOf(token) >= 0)
        return token;
    error("Something about a wrong token.");
}
```

Now, if we want to compile some math that looks like `max(max(1,2),max(3,4)))`, our call stack will go `parse_math` → `parse_function` → `parse_math` → `parse_function` → `parse_math` before we reach the `1`. But by doing the parsing like this, the recursion handles any "jumping around" you'd have to do without, and we can conceptually just walk from left to right. Handling parsing like this is called [recursive descent](https://en.wikipedia.org/wiki/Recursive_descent_parser).

IMAGE

These parse functions return syntax nodes, which all derive from an `ASTNode`[^6] base class. Usually, these have children that are also syntax nodes. For instance, the `ASTFunction` type above is defined as follows.

```js
class ASTFunction extends ASTNode {
    /** This node represents a function call.
     * @param {number} col
     * @param {string} name
     * @param {ASTNode[]} args
     */
    constructor(col, name, args) {
        super(col);
        this.name = name;
        this.args = args;
    }
}
```

This node has a function name, and a list of child nodes that, in order, represent the function arguments. This will give us the tree structure.

Continuing on, we implement everything with the following list of functions, all of which have a similar idea to the above, just with different structures.

```js
// Parse any math.
function parse_math() { .. }
// Parse a single term as part of arithmetic, such as "math+...".
function parse_term() { .. }
// Parse the "..." as in `parse_term()`'s comment.
// More on this below.
function parse_remaining_expression() { .. }
// Parse "+math" or "-math" unary operations.
function parse_unary() { .. }
// Parse a call like for instance "name(math,math)".
function parse_function() { .. }
// Parse a number (including `z` and `i` here.)
function parse_number() { .. }
```

Order of operations
===================
Now, the odd duck above is of course the `parse_remaining_expression()` function. When first writing a parser, you might be tempted to parse arithmetic the same way as you would do function arguments. Just go from left to right, and eat what you get, in order.

```js
// NOTE: This code is fundamentally wrong.
function parse_math() {
    // First parse the left-hand side.
    // This can only be a unary, function call, or number.
    let token = consume();
    let lhs = undefined;
    if (token.kind === TOKEN.Op
        && (token.value === "+" || token.value === "-"))
        lhs = parse_unary();
    else if (token.kind === TOKEN.Id)
        lhs = parse_function();
    else if (token.kind === TOKEN.Num)
        lhs = parse_number();
    else
        error("Something about malformed arithmetic.");

    // We're either part of an arithmetic expression, or done.
    if (!peek()) {
        return lhs;
    }

    // We're an arithmetic expression.
    // Now we can just consume the operator to know what we're doing.
    let op_token = consume_expected(TOKEN.Op);

    // The right-hand side may be anything.
    let rhs = parse_math();
    return new ASTOp(col, lhs, rhs, op_token.value);
}
```

Can you see the problem here? The title of this paragraph may be a hint. The order of operations is off. Consider the two bits of code `1+2*3**4` and `4**3*2+1`. This code generates the following two syntax trees.

IMAGE

However, exponentiation takes precedence over multiplication, which takes precedence over addition, so the TODO side is wrong. Unfortunately, we'll need to consider a bit more data to also take into account this precedence.

Analysis
========

Output
======

[/block]

[^1]:
    I support way too much math. Apart from the standard arithmetic operations, there's a whole lot more: `re`, `im`, `abs`, `sgn`, `normalize`, `ceil`, `floor`, `round`, `fract`, `clamp`, `max`, `min`, `avg`, `exp`, `ln`, `sqrt`, `cos`, `cosh`, `acos`, `acosh`, and the `sin` and `tan` variants of those. All of these are pretty self-explanatory.

    Of note is that $\log(z)$ is a [multivalued function](https://en.wikipedia.org/wiki/Multivalued_function). Because $e^{w + \alpha\pi} = e^{w + \alpha\pi i + 2k\pi i}$ for any $k \in \mathbb Z$, any complex value has multiple logarithms. I'm using the *principal branch*: the angle component that's giving us multivalued problems will be restricted to $(-\pi,\pi]$ to give us unique values again.

    This multivalued note also holds for the inverse trig functions. These can be [defined in terms of the principal logarithm](https://en.wikipedia.org/wiki/Inverse_trigonometric_functions#Logarithmic_forms), and that's exactly what I'm doing, so these pose no problem.

[^2]: The one I'm familiar with is "*An Introduction to Chaotic Dynamical Systems*" by Robert L. Devaney. It be good.
[^3]: Least significant footnote on this entire website, but I've only *just now* realized the similarities between "*lexer*" and the Dutch word "*lezer*" ("*reader*"). These words are only 1cm apart on the keyboard. Tsk.
[^4]:
    You can also automatically generate the tokenization and parsing steps. With these generators, you specify your language's syntax in [EBNF](https://en.wikipedia.org/wiki/Extended_Backus%E2%80%93Naur_form), and out rolls some code that turns text into a tree. Apparently Wikipedia has a [comprehensive comparison](https://en.wikipedia.org/wiki/Comparison_of_parser_generators) between these different parser generators.

    However, I like writing my parsers by hand more. I feel like it's less likely for me to screw up and create ambiguous syntax this way. (I still manage to screw it up sometimes, though.)

[^5]: Just to give you an idea: my very unfinished compiler from c# to mcfunction has around 37 different passes in this step (which excludes everything Roslyn does for me already!). Another compiler for a bullet hell project of mine has around 17 passes. Of course, proper professional compilers blow these numbers out of the water.
[^6]: There's not really a good place to put it in the main text, but `AST` stands for "*abstract syntax tree*". The "syntax tree" part is clear (you have a tree representing your syntax), but the "abstract" part comes from the fact that you're no longer dealing with literal text.

{:/nomarkdown}